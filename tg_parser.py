import asyncio
import hashlib
import logging
import os
import random
import re
from datetime import timezone
from typing import Any, Optional

import requests
from telethon import TelegramClient
from telethon.errors import FloodWaitError
from telethon.sessions import StringSession

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - tg_parser - %(levelname)s - %(message)s",
)
logger = logging.getLogger("tg_parser")


# -----------------------------
# ENV helpers
# -----------------------------
def _env_first(*names: str, default: str = "") -> str:
    for n in names:
        v = os.getenv(n)
        if v is not None and str(v).strip() != "":
            return str(v).strip()
    return default


# -----------------------------
# Miniapp config
# -----------------------------
API_BASE_URL = _env_first(
    "MINIAPP_URL",
    "miniapp_url",
    "API_BASE_URL",
    "API_URL",
    default="",
).rstrip("/")
if not API_BASE_URL:
    raise RuntimeError("MINIAPP_URL/miniapp_url is not set (URL of miniapp service)")


def _get_api_secret() -> str:
    return (
        os.getenv("API_SECRET")
        or os.getenv("MINIAPP_API_SECRET")
        or os.getenv("X_API_KEY")
        or os.getenv("PARSER_API_SECRET")
        or ""
    ).strip()


API_SECRET = _get_api_secret()


def _auth_headers() -> dict:
    headers: dict[str, str] = {}
    if API_SECRET:
        headers["X-API-KEY"] = API_SECRET
        headers["Authorization"] = f"Bearer {API_SECRET}"
    return headers


TG_GROUPS_API_URL = (os.getenv("TG_GROUPS_API_URL") or f"{API_BASE_URL}/api/groups").strip()

TG_API_ID = int(_env_first("TG_API_ID", "TG_API_ID_DEFAULT", "API_ID", default="0") or "0")
TG_API_HASH = _env_first("TG_API_HASH", "TG_API_HASH_DEFAULT", "API_HASH", default="")

# —Å–∫–æ–ª—å–∫–æ –Ω–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –º–∞–∫—Å–∏–º—É–º –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∑–∞ –æ–¥–∏–Ω —Ü–∏–∫–ª –Ω–∞ –æ–¥–∏–Ω –∏—Å—Ç–æ—á–Ω–∏–∫
TG_NEW_MESSAGES_LIMIT = int(_env_first("TG_NEW_MESSAGES_LIMIT", default="50") or "50")

# warm start: —á—Ç–æ–±—ã –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ –ù–ï —Ç–∞—â–∏–ª –∏—Å—Ç–æ—Ä–∏—é
WARM_START = (_env_first("TG_WARM_START", default="true").lower() in ("1", "true", "yes", "y"))

# polling
POLL_INTERVAL_SECONDS_RAW = _env_first("POLL_INTERVAL_SECONDS", default="").strip()
POLL_INTERVAL_MIN_SECONDS = int(_env_first("POLL_INTERVAL_MIN_SECONDS", default="60") or "60")
POLL_INTERVAL_MAX_SECONDS = int(_env_first("POLL_INTERVAL_MAX_SECONDS", default="180") or "180")

# keywords
JOB_KEYWORDS_RAW = _env_first("JOB_KEYWORDS", default="").strip()


def _parse_keywords(raw: str) -> list[str]:
    # –ø–æ–¥–¥–µ—Ä–∂–∏–º –∏ –∑–∞–ø—è—Ç—ã–µ, –∏ —Ç–æ—á–∫–∏ —Å –∑–∞–ø—è—Ç–æ–π, –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫
    raw = (raw or "").strip()
    if not raw:
        return []
    parts = re.split(r"[,\n;]+", raw)
    out = []
    for p in parts:
        p = p.strip().lower()
        if not p:
            continue
        out.append(p)
    # uniq preserving order
    return list(dict.fromkeys(out))


JOB_KEYWORDS = _parse_keywords(JOB_KEYWORDS_RAW)


def _next_sleep_seconds() -> int:
    if POLL_INTERVAL_SECONDS_RAW:
        try:
            return max(1, int(POLL_INTERVAL_SECONDS_RAW))
        except Exception:
            return 60
    lo = max(1, int(POLL_INTERVAL_MIN_SECONDS))
    hi = max(lo, int(POLL_INTERVAL_MAX_SECONDS))
    return random.randint(lo, hi)


# -----------------------------
# Optional GPT filter (OpenAI)
# -----------------------------
OPENAI_API_KEY = (os.getenv("OPENAI_API_KEY") or "").strip()
OPENAI_BASE_URL = (os.getenv("OPENAI_BASE_URL") or "https://api.openai.com/v1").rstrip("/")
OPENAI_MODEL = (os.getenv("OPENAI_MODEL") or "gpt-4o-mini").strip()
GPT_ENABLED = bool(OPENAI_API_KEY) and (_env_first("GPT_ENABLED", default="true").lower() in ("1", "true", "yes", "y"))

# –æ–≥—Ä–∞–Ω–∏—á–∏–º –¥–ª–∏–Ω—É —Ç–µ–∫—Å—Ç–∞ –≤ GPT, —á—Ç–æ–±—ã –Ω–µ –∂–µ—á—å —Ç–æ–∫–µ–Ω—ã
GPT_TEXT_MAX = int(_env_first("GPT_TEXT_MAX", default="2500") or "2500")


def gpt_is_relevant(text: str) -> tuple[bool, str]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (relevant, reason).
    –ï—Å–ª–∏ GPT –≤—ã–∫–ª—é—á–µ–Ω ‚Äî —Å—á–∏—Ç–∞–µ–º, —á—Ç–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ (–ø–æ—Å–ª–µ keyword-—Ñ–∏–ª—å—Ç—Ä–∞).
    """
    if not GPT_ENABLED:
        return True, "gpt_disabled"

    t = (text or "").strip()
    if len(t) > GPT_TEXT_MAX:
        t = t[:GPT_TEXT_MAX].rstrip() + "‚Ä¶"

    prompt = (
        "–¢—ã —Ñ–∏–ª—å—Ç—Ä –≤–∞–∫–∞–Ω—Å–∏–π.\n"
        "–û–ø—Ä–µ–¥–µ–ª–∏: —ç—Ç–æ –°–û–û–ë–©–ï–ù–ò–ï ‚Äî —Ä–µ–∞–ª—å–Ω–∞—è –≤–∞–∫–∞–Ω—Å–∏—è/–ø–æ–∏—Å–∫ —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞/–∑–∞–∫–∞–∑ –Ω–∞ —Ä–∞–±–æ—Ç—É?\n"
        "–í–∞–∂–Ω–æ–µ:\n"
        "- –ï—Å–ª–∏ —ç—Ç–æ –ø—Ä–æ—Å—Ç–æ –æ–±—Å—É–∂–¥–µ–Ω–∏–µ, –Ω–æ–≤–æ—Å—Ç–∏, –º–µ–º—ã, –±–æ–ª—Ç–æ–≤–Ω—è, —Å—Å—ã–ª–∫–∏ –±–µ–∑ –æ–ø–∏—Å–∞–Ω–∏—è ‚Äî –ù–ï —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ.\n"
        "- –ï—Å–ª–∏ —è–≤–Ω–æ –∏—â—É—Ç —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞/–∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—è/–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞/–º–µ–Ω–µ–¥–∂–µ—Ä–∞ –∏ —Ç.–ø. ‚Äî —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ.\n"
        "–û—Ç–≤–µ—Ç—å –°–¢–†–û–ì–û JSON –±–µ–∑ –ø–æ—è—Å–Ω–µ–Ω–∏–π:\n"
        '{"relevant": true/false, "reason": "–∫–æ—Ä–æ—Ç–∫–æ –ø–æ—á–µ–º—É"}\n\n'
        "–¢–µ–∫—Å—Ç:\n"
        f"{t}"
    )

    url = f"{OPENAI_BASE_URL}/chat/completions"
    headers = {
        "Authorization": f"Bearer {OPENAI_API_KEY}",
        "Content-Type": "application/json",
    }
    payload = {
        "model": OPENAI_MODEL,
        "temperature": 0,
        "messages": [
            {"role": "system", "content": "–¢—ã –∞–∫–∫—É—Ä–∞—Ç–Ω–æ –æ—Ç–≤–µ—á–∞–µ—à—å —Å—Ç—Ä–æ–≥–æ –≤–∞–ª–∏–¥–Ω—ã–º JSON."},
            {"role": "user", "content": prompt},
        ],
    }

    try:
        r = requests.post(url, headers=headers, json=payload, timeout=30)
        if r.status_code >= 400:
            return True, f"gpt_http_{r.status_code}"  # —á—Ç–æ–±—ã –Ω–µ –¥—Ä–æ–ø–∞—Ç—å –≤—Å—ë –∏–∑-–∑–∞ gpt
        data = r.json() or {}
        content = (
            (data.get("choices") or [{}])[0]
            .get("message", {})
            .get("content", "")
            .strip()
        )
        # –∏–Ω–æ–≥–¥–∞ –º–æ–¥–µ–ª—å –æ–±–æ—Ä–∞—á–∏–≤–∞–µ—Ç –≤ ```json
        content = re.sub(r"^```json\s*", "", content)
        content = re.sub(r"\s*```$", "", content)

        import json as _json

        obj = _json.loads(content)
        rel = bool(obj.get("relevant"))
        reason = str(obj.get("reason") or "").strip()[:200]
        return rel, reason or "ok"
    except Exception as e:
        # –µ—Å–ª–∏ GPT —É–ø–∞–ª ‚Äî –ª—É—á—à–µ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å, —á–µ–º —Å–ø–∞–º–∏—Ç—å ‚Äú–≤—Å—ë –Ω–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ‚Äù
        return True, f"gpt_error:{type(e).__name__}"


# -----------------------------
# Miniapp helpers
# -----------------------------
def send_alert(text: str) -> None:
    try:
        r = requests.post(
            f"{API_BASE_URL}/api/alert",
            headers=_auth_headers(),
            json={"text": text, "message": text, "source": "tg_parser"},
            timeout=10,
        )
        if r.status_code >= 400:
            logger.error("‚ùå /api/alert failed http=%s body=%s", r.status_code, r.text[:800])
    except Exception:
        logger.exception("‚ùå /api/alert exception")


def post_status(key: str, value: str) -> None:
    try:
        r = requests.post(
            f"{API_BASE_URL}/api/parser_status/{key}",
            json={"value": value},
            headers=_auth_headers(),
            timeout=10,
        )
        if r.status_code >= 400:
            logger.error("‚ùå /api/parser_status/%s failed http=%s body=%s", key, r.status_code, r.text[:800])
    except Exception:
        logger.exception("‚ùå /api/parser_status exception")


def fetch_tg_session_from_miniapp() -> str:
    """miniapp endpoint: GET /api/parser_secrets/tg_session -> {value: "..."}"""
    if not API_SECRET:
        return ""
    url = f"{API_BASE_URL}/api/parser_secrets/tg_session"
    try:
        r = requests.get(url, headers=_auth_headers(), timeout=10)
        if r.status_code >= 400:
            return ""
        data = r.json() or {}
        return (data.get("value") or "").strip()
    except Exception:
        return ""


def _looks_like_telegram(raw: str) -> bool:
    s = (raw or "").strip().lower()
    if not s:
        return False
    if s.startswith("@"):
        return True
    if "t.me/" in s or "telegram.me/" in s:
        return True
    if re.fullmatch(r"[a-zA-Z0-9_]{4,}", s):
        return True
    if re.fullmatch(r"-?\d+", s):
        return True
    return False


def fetch_telegram_sources() -> list[str]:
    try:
        r = requests.get(TG_GROUPS_API_URL, headers=_auth_headers(), timeout=10)
        if r.status_code >= 400:
            logger.error("‚ùå groups fetch failed http=%s body=%s", r.status_code, r.text[:300])
            return []
        data = r.json() or {}
        groups = data.get("groups") or []
    except Exception as e:
        logger.error("‚ùå groups fetch exception: %s", e)
        return []

    out: list[str] = []
    for g in groups:
        if not isinstance(g, dict):
            continue
        if not g.get("enabled", True):
            continue
        t = (g.get("type") or "").lower().strip()
        if t and t != "telegram":
            continue
        raw = (g.get("group_id") or "").strip()
        if not raw:
            continue
        if not _looks_like_telegram(raw):
            continue
        out.append(raw)

    return list(dict.fromkeys(out))


def _normalize_tg_source(raw: str) -> str:
    s = (raw or "").strip()
    if not s:
        return ""

    m = re.search(r"(?:t\.me|telegram\.me)/([a-zA-Z0-9_+\-]+)/?", s)
    if m:
        tail = m.group(1)
        if tail.startswith("+"):
            # invite link: –±–µ–∑ join –æ–±—ã—á–Ω–æ –Ω–µ –≤—ã—Ç–∞—â–∏–º. –û—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å.
            return s
        return "@" + tail.lstrip("@")

    if re.fullmatch(r"[a-zA-Z0-9_]{4,}", s) and not s.startswith("@"):
        return "@" + s

    return s


def _external_id_from_message(msg: Any) -> str:
    pid = None
    try:
        peer = getattr(msg, "peer_id", None)
        if peer is not None:
            pid = getattr(peer, "channel_id", None) or getattr(peer, "chat_id", None) or getattr(peer, "user_id", None)
    except Exception:
        pid = None
    return f"{pid or 'unknown'}:{getattr(msg, 'id', None)}"


def _hash_fallback(text: str, created_at: Optional[str]) -> str:
    base = (text or "").strip() + "|" + (created_at or "")
    return hashlib.sha256(base.encode("utf-8", "ignore")).hexdigest()


def send_job_to_miniapp(
    text: str,
    external_id: str,
    url: Optional[str],
    created_at: Optional[str],
    source_name: str,
    sender_username: Optional[str],
) -> None:
    payload = {
        "source": "telegram",
        "source_name": source_name,
        "external_id": external_id or _hash_fallback(text, created_at),
        "url": url,
        "text": text,
        "sender_username": sender_username,
        "created_at": created_at,
    }

    r = requests.post(f"{API_BASE_URL}/post", json=payload, headers=_auth_headers(), timeout=30)
    if r.status_code != 200:
        logger.error("‚ùå /post failed: http=%s body=%s", r.status_code, r.text[:800])
        send_alert(f"TG parser: /post failed\nHTTP {r.status_code}\n{r.text[:800]}")
        r.raise_for_status()


# -----------------------------
# Keyword match
# -----------------------------
def _keyword_match(text: str) -> bool:
    if not JOB_KEYWORDS:
        return False  # –≤–∞–∂–Ω–æ: –µ—Å–ª–∏ keywords –Ω–µ –∑–∞–¥–∞–Ω—ã ‚Äî –ù–ò–ß–ï–ì–û –Ω–µ —à–ª—ë–º (–∏–Ω–∞—á–µ –±—É–¥–µ—Ç ‚Äú–≤–µ—Å—å —á–∞—Ç‚Äù)
    t = (text or "").lower()
    return any(k in t for k in JOB_KEYWORDS)


# -----------------------------
# Parsing
# -----------------------------
async def _parse_one_source(
    client: TelegramClient,
    source_raw: str,
    last_ids: dict[str, int],
) -> tuple[int, int, int]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (new_msgs_seen, matched_by_keywords, sent_after_gpt)
    """
    source = _normalize_tg_source(source_raw)
    if not source:
        return 0, 0, 0

    try:
        entity = await client.get_entity(source)
    except Exception as e:
        logger.warning("‚ö†Ô∏è cannot resolve source=%r: %s", source_raw, e)
        return 0, 0, 0

    username = getattr(entity, "username", None)
    title = getattr(entity, "title", None) or getattr(entity, "first_name", None) or source_raw
    source_name = str(title)

    # –∫–ª—é—á –¥–ª—è —Å–ª–æ–≤–∞—Ä—è last_ids –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å—Ç–∞–±–∏–ª—å–Ω—ã–π
    entity_key = source_name + "|" + (username or source)

    # warm start: –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ ‚Äî –∑–∞–ø–æ–º–∏–Ω–∞–µ–º —Ç–µ–∫—É—â–∏–π top id –∏ –ù–ï —à–ª—ë–º –∏—Å—Ç–æ—Ä–∏—é
    if entity_key not in last_ids and WARM_START:
        try:
            latest = await client.get_messages(entity, limit=1)
            if latest and latest[0]:
                last_ids[entity_key] = int(latest[0].id)
                logger.info("üî• warm_start: %s last_id=%s", source_name, last_ids[entity_key])
                return 0, 0, 0
        except Exception:
            last_ids[entity_key] = 0

    min_id = int(last_ids.get(entity_key, 0) or 0)

    new_seen = 0
    kw_matched = 0
    sent = 0
    max_id_seen = min_id

    try:
        # reverse=True => –æ—Ç —Å—Ç–∞—Ä—ã—Ö –∫ –Ω–æ–≤—ã–º (—É–¥–æ–±–Ω–æ –æ–±–Ω–æ–≤–ª—è—Ç—å last_id)
        async for msg in client.iter_messages(entity, min_id=min_id, limit=TG_NEW_MESSAGES_LIMIT, reverse=True):
            new_seen += 1
            if msg.id and int(msg.id) > max_id_seen:
                max_id_seen = int(msg.id)

            text = (getattr(msg, "message", None) or getattr(msg, "text", None) or "").strip()
            if not text:
                continue

            if not _keyword_match(text):
                continue
            kw_matched += 1

            # GPT refine
            ok, reason = gpt_is_relevant(text)
            if not ok:
                logger.info("üßπ gpt_drop (%s): %s", source_name, reason)
                continue

            dt = getattr(msg, "date", None)
            created_at = None
            if dt:
                try:
                    if dt.tzinfo is None:
                        dt = dt.replace(tzinfo=timezone.utc)
                    created_at = dt.astimezone(timezone.utc).isoformat()
                except Exception:
                    created_at = None

            url = None
            if username:
                url = f"https://t.me/{username}/{msg.id}"

            sender_username = None
            try:
                sender = await msg.get_sender()
                sender_username = getattr(sender, "username", None)
                if sender_username:
                    sender_username = "@" + sender_username.lstrip("@")
            except Exception:
                pass

            ext_id = _external_id_from_message(msg)
            send_job_to_miniapp(
                text=text,
                external_id=ext_id,
                url=url,
                created_at=created_at,
                source_name=source_name,
                sender_username=sender_username,
            )
            sent += 1

    except FloodWaitError as e:
        wait_s = int(getattr(e, "seconds", 0) or 0)
        logger.warning("‚è≥ FloodWait on %s: %ss", source_name, wait_s)
        await asyncio.sleep(wait_s + 1)
    except Exception as e:
        logger.warning("‚ö†Ô∏è parse failed (%s): %s", source_name, e)

    # –æ–±–Ω–æ–≤–ª—è–µ–º last_id –¥–∞–∂–µ –µ—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –æ—Ç–ø—Ä–∞–≤–∏–ª–∏ ‚Äî –∏–Ω–∞—á–µ —Å–Ω–æ–≤–∞ –±—É–¥–µ–º –≥–æ–Ω—è—Ç—å –æ–¥–Ω–æ –∏ —Ç–æ –∂–µ
    last_ids[entity_key] = max(max_id_seen, min_id)
    return new_seen, kw_matched, sent


async def main() -> None:
    if not TG_API_ID or not TG_API_HASH:
        raise RuntimeError("TG_API_ID/TG_API_HASH is not set")

    if not JOB_KEYWORDS:
        msg = (
            "TG parser: JOB_KEYWORDS –ø—É—Å—Ç–æ–π ‚Äî —á—Ç–æ–±—ã –Ω–µ —Å–ø–∞–º–∏—Ç—å, –ø–∞—Ä—Å–µ—Ä –ù–ò–ß–ï–ì–û –Ω–µ –±—É–¥–µ—Ç –æ—Ç–ø—Ä–∞–≤–ª—è—Ç—å.\n"
            "–ó–∞–ø–æ–ª–Ω–∏ JOB_KEYWORDS –≤ Railway (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é)."
        )
        logger.error(msg)
        post_status("tg_parser", "no_keywords")
        send_alert(msg)

    last_ids: dict[str, int] = {}

    last_session = ""
    client: TelegramClient | None = None

    logger.info("‚úÖ tg_parser started (warm_start=%s, keywords=%s, gpt=%s)", WARM_START, len(JOB_KEYWORDS), GPT_ENABLED)
    send_alert("‚úÖ tg_parser started.")

    while True:
        session_str = fetch_tg_session_from_miniapp() or _env_first("TG_SESSION", "TELEGRAM_SESSION", "SESSION", default="")
        session_str = (session_str or "").strip()

        if not session_str:
            msg = (
                "TG parser: StringSession –ø—É—Å—Ç–∞—è.\n"
                "–û—Ç–∫—Ä–æ–π –º–∏–Ω–∏–∞–ø–ø ‚Üí ‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ‚Üí Telegram –∏ —Å–æ–∑–¥–∞–π/–≤—Å—Ç–∞–≤—å StringSession."
            )
            logger.error(msg)
            post_status("tg_parser", "no_session")
            send_alert(msg)
            await asyncio.sleep(60)
            continue

        if client is None or session_str != last_session:
            if client is not None:
                try:
                    await client.disconnect()
                except Exception:
                    pass
            client = TelegramClient(StringSession(session_str), TG_API_ID, TG_API_HASH)
            await client.connect()
            last_session = session_str

        assert client is not None

        try:
            if not await client.is_user_authorized():
                msg = "TG parser: —Ç–µ–∫—É—â–∞—è StringSession –Ω–µ –∞–≤—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–∞ (not_authorized)."
                logger.error(msg)
                post_status("tg_parser", "not_authorized")
                send_alert(msg)
                await asyncio.sleep(120)
                continue

            sources = fetch_telegram_sources()
            if not sources:
                logger.warning("‚ö†Ô∏è sources –ø—É—Å—Ç—ã–µ ‚Äî –ø–∞—Ä—Å–∏—Ç—å –Ω–µ—á–µ–≥–æ")
                post_status("tg_parser", "no_sources")
            else:
                total_new = 0
                total_kw = 0
                total_sent = 0

                # –µ—Å–ª–∏ keywords –ø—É—Å—Ç—ã–µ ‚Äî –ø—Ä–æ—Å—Ç–æ —á–∏—Ç–∞–µ–º –Ω–æ–≤—ã–µ ids, –Ω–æ –Ω–µ —à–ª—ë–º
                if not JOB_KEYWORDS:
                    for s in sources:
                        await _parse_one_source(client, s, last_ids)
                    post_status("tg_parser", f"no_keywords sources={len(sources)}")
                else:
                    for s in sources:
                        new_seen, kw_matched, sent = await _parse_one_source(client, s, last_ids)
                        total_new += new_seen
                        total_kw += kw_matched
                        total_sent += sent

                    post_status("tg_parser", f"ok new={total_new} kw={total_kw} sent={total_sent} sources={len(sources)}")
                    logger.info("‚úÖ cycle: new=%s kw=%s sent=%s sources=%s", total_new, total_kw, total_sent, len(sources))

        except FloodWaitError as e:
            wait_s = int(getattr(e, "seconds", 0) or 0)
            logger.warning("‚è≥ FloodWait (global): %ss", wait_s)
            post_status("tg_parser", f"flood_wait {wait_s}s")
            await asyncio.sleep(wait_s + 1)
        except Exception as e:
            logger.exception("‚ùå unexpected error")
            post_status("tg_parser", "error")
            send_alert(f"TG parser error: {e}")

        sleep_s = _next_sleep_seconds()
        logger.info("‚è≤Ô∏è sleep %ss", sleep_s)
        await asyncio.sleep(sleep_s)


if __name__ == "__main__":
    asyncio.run(main())
